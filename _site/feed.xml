<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-01-01T03:35:19+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Credit Risk Data Science - A Pocket Book</title><subtitle>Another kind of simplified manual for credit risk data scientist.</subtitle><entry><title type="html">Logistic Regression Modelling</title><link href="http://localhost:4000/2025/01/01/logistic-regression-modelling.html" rel="alternate" type="text/html" title="Logistic Regression Modelling" /><published>2025-01-01T00:00:00+07:00</published><updated>2025-01-01T00:00:00+07:00</updated><id>http://localhost:4000/2025/01/01/logistic-regression-modelling</id><content type="html" xml:base="http://localhost:4000/2025/01/01/logistic-regression-modelling.html"><![CDATA[<h1 id="developing-credit-risk-scorecard-using-logistic-regression">Developing Credit Risk Scorecard Using Logistic Regression</h1>
<p>Logistic regression is a statistical model that is commonly used for binary classification tasks. It’s elegance lies in its simplicity and interpretability, making it suitable for credit risk models such as PD model.</p>

<p>This section explains the basic mechanics behind logistic regression, use it to develop a predictive model, and convert it into a scorecard.</p>

<h2 id="understanding-the-mathematical-formulation">Understanding the mathematical formulation</h2>
<p>The foundation of logistic regression lies in its ability to estimate probabilities using a logistic function, which is an S-shaped curve also known as the sigmoid function. The logistic function models the probability that the dependent variable belongs to a particular category.</p>

<p>For a binary classificationt ask, let $Y$ represents a binary target where $Y$ is either $1$ $(True)$ or $0$ $(False)$, and $X$ represents the features or predictor variables, a logistic regression model predicts the probability $P(Y=1|X)$ as:</p>

\[P(Y = 1 | X) = \frac{1}{1 + e^{-(f(x))}}\]

<p>where</p>

\[f(x)=\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]

<p>, $\beta_0,\beta_1,\beta_2,…,\beta_k$ are the model coefficients, and $e$ is the natural logiarithm base. The model coffecients are learnt during the model training, which is basically solving an optimization problem of the following function:</p>

\[\beta^* = \arg \min_{\beta} -\left[ \sum_{i=1}^n \left( y_i \log \left(\frac{1}{1 + e^{-\beta^T x_i}}\right) + (1 - y_i) \log \left(1 - \frac{1}{1 + e^{-\beta^T x_i}}\right) \right) \right]\]

<p>where $y_i$ is the observed binary outcome of the $i$-th observation (either 1 or 0), $x_i$ is the vector predictor variables  for the $i$-th observation, and $\beta$ is the model coefficients.</p>

<p>Let’s leave the optimization problem and focus on the logistic regression formula. You can see from the relationship between $P(Y=1|X)$ and $f(x)$ that extremly high value of $f(X)$ will make the $P(Y=1|X)$ close to 1, and extremly low value of $f(X)$ will make the $P(Y=1|X)$ close to 0. In fact, $X$ is linear to to the log-odd probability of $Y$. Or we can also at least the relationship between $X$ and $P(Y=1|X)$ is monotonic. This is the first assumption of logistic regression model. Unfortunately, in reality, this is often not the case. Therefore, we need to transform the input variables before we can feed them into the logisti regression model for more optimum result.</p>

<p>The second assumption of the logistic regression model is that the predictors are independent. i.e, the values between predictors, $x_0,x_1,…,x_n$, should not be correlated. You can see from $f(X)$ that each predictor $x_i$ has its own beta coefficient $\beta_i$, and the result of $f(X)$ is basically the summation of $\beta_ix_i$. High correlation between predictor variables can lead to difficulties in estimating the model coefficients because it becomes hard to disentangle the effect of each variable.</p>

<p>There are also other reasonable assumption in logistic regression model such as there should be no combination of predictor variables that can perfectly predicts the binary output (quasi-complete separation), and the observation $i,i+1,i+2,…,n$, should be independent eachother. Quasi-complete separation can to problems in the estimation process, typically resulting in extremely large coefficient estimates or failure to converge.</p>

<h2 id="strength-and-weaknesses-of-logistic-regression-model">Strength and weaknesses of logistic regression model</h2>
<p>One of the logistic regression’s primary strengths is its interpretability; the output of a logistic regression model is a probability that gives clear insight into the relationship between the independent variables and the outcome. This interpretability extends to its coefficients, which represent the change in the log odds of the dependent variable per unit change in the predictor. This aspect makes logistic regression valuable in fields like finance, medicine, and social sciences, where understanding the influence of variables is crucial.</p>

<p>Another advantage of logistic regression is its simplicity and efficiency in terms of computation, particularly when compared to more complex models like neural networks or ensemble methods (including random forest and gradient boosted trees). It doesn’t require high computational resources, making it suitable for situations with constrained computational capacity.</p>

<p>However, logistic regression also has its limitations. The assumption of linearity between the logit of the outcome and each predictor can be a drawback, as it restricts the model’s ability to capture more complex relationships in the data. Additionally, as logistic regression assumes between-predictors independency, the model struggles to capture interraction between predictors.</p>

<p>Logistic regression also struggles with problems of multicollinearity among predictors, where high correlations between variables can destabilize the coefficient estimates, making them difficult to interpret. Another weakness is the impact of outliers and influential points, which can disproportionately affect the model due to the model’s reliance on maximum likelihood estimation.</p>

<p>Compared to more modern ensemble models such as gradient boosted trees, logistic regression lack of boosting capability. Boosting can boost the model performance by minimizing the errors on the part of data that has high degree of false prediction. This capability, combined with bigger size of data, can significantly improve the performance of the model. Additionally, this capability also allow the model to use more predictors without losing significant performance gain.</p>

<h2 id="feature-importance-in-logistic-regression">Feature importance in logistic regression</h2>
<p>The most straightforward method to assess feature importance in logistic regression is by examining the magnitude and sign of the model coefficients. In logistic regression, each coefficient represents the change in the log odds of the outcome for a one-unit change in the corresponding feature, assuming other variables are held constant.</p>

<p>In the case of coefficient’s magnitude, larger absolute values of coefficients indicate a greater impact of the corresponding feature on the outcome. A larger positive coefficient increases the log odds of the outcome, suggesting a strong positive association with the dependent variable. Conversely, a larger negative coefficient decreases the log odds, indicating a strong negative influence.</p>

<p>For the sign of the coefficient (positive or negative), it indicates the direction of the association between the feature and the outcome. Positive coefficients increase the probability of the outcome occurring, while negative coefficients decrease it.</p>

<p>To compare the relative importance of variables that are on different scales, it is useful to look at standardized coefficients. Standardizing involves scaling the coefficients by their standard errors, which adjusts for the scale of the variables and allows for a more direct comparison across different features. This can be particularly important when dealing with variables that vary in units or range, such as comparing age in years to income in thousands of dollars.</p>

<p>Standardization, scaling, or transformation can also be done at the predictor level before their inclusion in the model. This will ensure every predictor has the same range. A notable method for this purpose is the Weight of Evidence (WoE) transformation, which is extensively used in credit risk modeling. WoE transformation does not only standardize the predictors. It also handles outliers, convert categorical variables into numbers, and effectively solve issues on moderate target imbalance which is commonly found in PD model development.</p>

<h2 id="regularization-in-logistic-regression">Regularization in logistic regression</h2>
<p>Regularization is a technique used in the context of predictive models to prevent overfitting and enhance model generalizability. In the case of logistic regression, there are three types regularizations: L1, L2, and elastic net.</p>

<h3 id="l1-regularization-lasso">L1 Regularization (Lasso)</h3>
<p>L1 regularization adds a penalty equivalent to the absolute value of the magnitude of coefficients to the objective function. This can lead to some coefficients being zeroed out, making it useful for feature selection in models with many features. The regularization term for L1 is given by:</p>

\[Penalty_{L1} = \lambda \sum_{j=1}^{p} |\beta_j|\]

<p>hence the (simplified) objective function becomes:</p>

\[\beta^* = \arg \min_{\beta} (-\left[ \sum_{i=1}^{n} \left( y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right) \right] + \lambda \sum_{j=1}^{p} |\beta_j|)\]

<p>where $\lambda$ is the L1 regularization strength, $\beta$ is the predictor, and $P$ is the number of predictors. As you can see from the penalized objective function, bigger the $\beta$ coefficients give bigger penalty to the function that should  be minimized by the solver.</p>

<h3 id="l2-egularization-ridge">L2 egularization (Ridge)</h3>
<p>L2 regularization adds a penalty equivalent to the square of the magnitude of coefficients. This effectively shrinks the coefficients and helps to handle multicollinearity by keeping all variables in the model but penalizing their values if they are too large. The regularization term for L2 is</p>

\[Penalty_{L2} = \lambda \sum_{j=1}^{p} \beta^2_j\]

<p>. As you can see from the regularization term, there is a quadratic term that produce stronger effect, especially when value of $\beta$ is high.</p>

<h3 id="elastic-net">Elastic Net</h3>
<p>Elastic Net combines the penalties of L1 and L2 regularization. Elastic Net aims to enjoy the benefits of both Ridge and Lasso regularization. The regularization term is</p>

\[Penalty_{L1+L2} = r\lambda \sum_{j=1}^{p} |\beta_j| + (1-r)\lambda \sum_{j=1}^{p} \beta^2_j\]

<p>where $r$ is the regularization ratio between L1 and L2.</p>

<h2 id="comparison-of-logistic-regression-with-ensemble-models">Comparison of logistic regression with ensemble models</h2>
<p>Logistic regression, while robust and widely utilized, does not incorporate boosting, a technique often employed in more modern ensemble models like gradient boosted trees. Boosting is a powerful method that enhances model performance by iteratively focusing on the portions of the data where prediction errors are high. This targeted improvement helps in refining the model’s accuracy over successive iterations.</p>

<p>Ensemble models that use boosting are particularly adept at handling large datasets and can effectively incorporate a greater number of predictors without a corresponding loss in performance. This capability allows these models to adapt more flexibly to complex datasets and capture subtle patterns that may be missed by simpler models like logistic regression.</p>

<p>In contrast, logistic regression’s performance is generally constrained by its linear nature and the absence of mechanisms to iteratively refine predictions based on previous errors. As a result, while logistic regression is valuable for its interpretability and efficiency, it may not achieve the same level of accuracy as boosted ensemble models when dealing with complex or large-scale data environments. This distinction is crucial for practitioners to consider when selecting the appropriate modeling approach for their specific data and analytical needs.</p>

<p>Developing predictive models using logistic regression usually involves more manual steps compared to developing predictivemodels using ensemble models like gradient boosted trees. Some common manual steps include introducing feature interraction, transforming predictors to align with the log-odd linearity wth the outcome, limitting the predictor variables by selecting only the best performing ones, etc.</p>

<h2 id="introducing-feature-interraction">Introducing feature interraction</h2>
<p>placeholder</p>

<h2 id="weight-of-evidence-woe-transformation">Weight of Evidence (WoE) transformation</h2>
<p>Placeholder</p>

<h2 id="feature-selection">Feature selection</h2>
<p>Placeholder</p>

<h2 id="hyperparameter-tuning">Hyperparameter tuning</h2>
<p>Placeholder</p>

<h2 id="converting-logistic-regression-model-into-a-scorecard">Converting logistic regression model into a scorecard</h2>
<p>Placeholder</p>

<h2 id="monitoring-the-model">Monitoring the model</h2>
<p>Placeholder</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Developing Credit Risk Scorecard Using Logistic Regression Logistic regression is a statistical model that is commonly used for binary classification tasks. It’s elegance lies in its simplicity and interpretability, making it suitable for credit risk models such as PD model.]]></summary></entry><entry><title type="html">Abc Scores And Stress Testing Introduction</title><link href="http://localhost:4000/2025/01/01/abc-scores-and-stress-testing-introduction.html" rel="alternate" type="text/html" title="Abc Scores And Stress Testing Introduction" /><published>2025-01-01T00:00:00+07:00</published><updated>2025-01-01T00:00:00+07:00</updated><id>http://localhost:4000/2025/01/01/abc-scores-and-stress-testing-introduction</id><content type="html" xml:base="http://localhost:4000/2025/01/01/abc-scores-and-stress-testing-introduction.html"><![CDATA[<h2 id="abc-scores">ABC Scores</h2>
<p>In credit risk terms, ABC scores refer to models that assess the creditworthiness of potential or existing borrowers at three different point of time. Application score (A) is the initial score used to decide whether the loan application of potential borrowers will be approved or rejected based on their data that is available at the time of application such as applicant’s credit history, income, employment status, and other relevant financial information. The data usually originates from credit bureau, government organization, and even other third party data vendors.</p>

<p>A score, is typically derived from a logistic regression model that predicts whether the borrowers will have payment defaults in a spevified time horizon or not. This type of model is known as a Payment Default (PD) model. Once the model is trained, it is then transformed into an easy-interpretable scorecard. Nowadays, due to the advancement of technology that supports data science including increased data size and better computational power, machine learning (ML) models such as random forest and gradient boosted trees are also commonly used to develop A score model.</p>

<p>The Behavior (B) score, like A score, is also part of Payment Default (PD) model. While the A score is used during the initial credit application process, the B score is used once a line of credit has been granted. It serves to continually monitor and assess the borrower’s creditworthiness as they manage their account. This includes tasks such as adjusting credit limits, modifying interest rates, or proposing alternative payment options to prevent defaults.</p>

<p>Furthermore, B scores are also commonly used in the Expected Credit Loss (ECL) calculation, marketing efforts and cross-selling, tailoring collection strategies, and ensuring regulatory compliance and reporting. Due to these diverse applications, the B score is frequently updated to reflect the most recent data snapshot, ensuring that it provides a current and accurate measure of credit risk.</p>

<p>The collection (C) score is used for accounts with payment delinquency. This score helps financial institutions prioritize their collection efforts based on the likelihood of recovering debts. Accounts with a higher likelihood of payment can be treated with less aggressive and costly collection strategies, while those with lower scores might need more focused attention.</p>

<p>The C score, which is also another output of predictive model, is more closely aligned with the Loss Given Default (LGD) model rather than the Payment Default (PD) model. We will explore LGD model in more details in the section dedicated to the ECL.</p>

<h2 id="ecl-calculation">ECL calculation</h2>
<p>One of the latest development of credit risk management is the adoption of ECL calculation, particularly under IFRS 9 standards. ECL calculation is used by the financial institutions to estimate the credit loss of their financial assets such as loans and bonds. The output of ECL calculation can be used as an early recognition of credit losses or to estimate profitability. Additionally, the result of ECL is also reported to the regulator and can influence the proportion of assets that financial institutions are permitted to loan out, impacting their lending capacity either positively or negatively.</p>

<p>ECL calculation, according to the IFRS 9 standards, consist of three main components: Probability of Defualt (PD), Lost Given Default (LGD), and Exposure at Default (EAD). The PD model predict the probability of a borrower will within a given time horizon. In order to calculate ECL, usually two time horizons are used depending on the asset’s risk level: 12 months PD and lifetime PD.</p>

<p>LGD model estimates the economic loss if the default occurs, considering the recovery rates on collateral or other credit enhancements. LGD is expressed as a percentage and represents the loss rate on an exposure if a default occurs. It is calculated as the difference between the total exposure at the time of default and the recovery amount (post-recovery costs), divided by the total exposure at default.</p>

<p>EAD model estimates the amount that a financial institution is exposed to when a default occurs. It includes not only the outstanding balance at the time of default but also any additional amounts that may be drawn down before the default occurs, such as undrawn credit lines or committed but undrawn loans.</p>

<h2 id="stress-testing">Stress testing</h2>
<p>In credit risk management, stress testing is a method to evaluate how financial instituion’s operations and portfolios respond to severe but plausible adverse economic scenarios. These tests are designed to simulate the impact of various extreme conditions, such as economic downturns, market crashes, or other disruptive events. The financial institutions use stress testing to understand potential vulnerabilities and prepare for unexpected losses.</p>

<p>Regulators in many countries require  financial institutions to perform regular stress tests to ensure that they can withstand economic shocks. Additionally, from the perspective of financial institutions, stress testing helps the management to make informed decisions regarding capital planning, risk appetite, and growth strategies.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[ABC Scores In credit risk terms, ABC scores refer to models that assess the creditworthiness of potential or existing borrowers at three different point of time. Application score (A) is the initial score used to decide whether the loan application of potential borrowers will be approved or rejected based on their data that is available at the time of application such as applicant’s credit history, income, employment status, and other relevant financial information. The data usually originates from credit bureau, government organization, and even other third party data vendors.]]></summary></entry></feed>